{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlays text of classification from model.\n",
    "def overlayText(txt,x,y):\n",
    "    cv2.putText(img, txt, (x,y), cv2.FONT_HERSHEY_TRIPLEX,1,(0,255,0))\n",
    "    \n",
    "# Format face in bounds.\n",
    "def apply_offsets(face_coordinates, offsets):\n",
    "    x, y, width, height = face_coordinates\n",
    "    x_off, y_off = offsets\n",
    "    return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "hi\n",
      "hi me\n"
     ]
    }
   ],
   "source": [
    "# Built-in Camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Facial detector from OpenCV.\n",
    "face_cascade = cv2.CascadeClassifier('/Users/mattmartin/Desktop/DS 4440/Project/motion/data/haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "# Our model.\n",
    "emotion_model_path = '/Users/mattmartin/Desktop/DS 4440/Project/motion/data/vgg16_64.h5'\n",
    "\n",
    "\n",
    "# Load Model\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "\n",
    "emotion_offsets = (20, 40)\n",
    "ID = -1\n",
    "frame_counter = 0\n",
    "face_counter = 0\n",
    "\n",
    "# # Map emotion IDs to Names\n",
    "emotion_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy',\n",
    "                4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
    "\n",
    "emotions_seen = []\n",
    "\n",
    "run = True\n",
    "while (run):\n",
    "    cur_faces = 0\n",
    "    frame_counter += 1\n",
    "    if (frame_counter % 1) == 0:\n",
    "        ret, img = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_image,1.3,5)\n",
    "\n",
    "        for (x,y,w,h) in faces:\n",
    "            img = cv2.rectangle(img,(x, y),(x+w, y+h),(48,0,0),2)\n",
    "            x1, x2, y1, y2 = apply_offsets((x,y,w,h), emotion_offsets)\n",
    "            cur_faces = len(faces)\n",
    "            \n",
    "            gray_face = gray_image[y1:y2, x1:x2]\n",
    "            try:\n",
    "                gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            gray_face = np.expand_dims(gray_face, 0)\n",
    "\n",
    "            gray_face = np.expand_dims(gray_face, -1)\n",
    "\n",
    "            emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "            emotion_label_arg = np.argmax(emotion_prediction)\n",
    "            emotion_text = emotion_labels[emotion_label_arg]\n",
    "            emotions_seen.append(emotion_text)\n",
    "            overlayText(emotion_text, x, y)\n",
    "\n",
    "            \n",
    "        # Displays the number of faces currently in the screen\n",
    "        overlayText(\"Current Faces: \" + str(cur_faces), 920, 80)\n",
    "        # Display the current frame\n",
    "        cv2.imshow('Live Recording', img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            run = False\n",
    "            break\n",
    "            \n",
    "                    \n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
